# SemanticSegmentation
A Semantic Segmentation project using CAM101 dataset and Deep Learning Algorithms; namely Fully Convolution Network, UNet and DeepLab Series
<br>
The Cambridge Labelled Objects in Video dataset was used in this project. It is made up of 101 960x720-pixel photos in which every pixel has been manually labelled with one of 32 object classifications that are pertinent to driving environments, such as pedestrians, automobiles, and traffic signs. The label_colors.txt file, which is also part of the dataset, links each R, G, and B colour value to the appropriate item class name. <br>
Semantic segmentation using deep learning models is the task in this research. The objective is to classify each pixel in an image into a certain object class from a list of predefined classifications.<br>
Three model architectures for semantic segmentation will be investigated:
<ol>
<li>Fully Convolutional Networks (FCN) (e.g., FCN-16s, FCN-8s)</li>
<li>UNet</li>
<li>DeepLab series (e.g., DeepLabV2, DeepLabV3, DeepLabV3+)</li>
</ol>
We will use the PyTorch package to implement each model structure and train them using the labelled picture dataset. To increase the dataset's size and the model's adaptability to new data, we will also extend it using the Albumentations library.
Design Rationale<br>
The hyperparameters that have been tweaked in the provided code are the learning rate and total number of epochs. In particular, the model is trained for 20 epochs with a learning rate of 0.001. Some hyperparameters and methods that could be used or tweaked to better investigate and enhance the performance of the model are:
<ol>
<li>Batch size: Changing the batch size can impact both the model's performance and how quickly it trains. Faster convergence can be achieved with smaller batch sizes, but training noise may increase. Although they demand more memory, large batch sizes can produce more reliable gradient estimates.</li>
<li>Loss function: A variety of loss functions, including Dice Coefficient, Weighted Pixel Cross-entropy, and Pixel-wise Cross-entropy, may be utilised. The performance of the model can be significantly impacted by selecting an appropriate loss function.</li>
<li>Optimizer: Various optimisation techniques, including Adam, SGD, and RMSprop, may be employed. The training procedure and performance can both be significantly impacted by selecting the right optimizer.</li>
<li>Data augmentation: To enhance the model's capacity to generalise to new data, various augmentation strategies might be investigated. You can add enhancements like horizontal and vertical flips, random rotations, and colour jittering with the Albumentations library.</li>
<li>Model architecture: In addition to the UNet model used in the provided code, other model architectures like PSPNet or DeepLab can be investigated and contrasted. Performance can also be impacted by adding skip connections and altering the depth or width of the design.</li>
<li>Fine-tuning: A pre-trained model can be loaded and modified for the task rather than having to train the model from scratch. Performance may vary depending on the pre-trained model and the layers that are unfrozen.</li>
<li>Regularisation: To avoid overfitting and increase the model's capacity for generalisation, strategies like dropout, weight decay, and batch normalisation can be added.</li>
</ol>
Here is a detailed analysis of how different hyperparameters and techniques can affect the performance of the model:
<ol>
<li>Batch size: Although it needs more memory, increasing the batch size can result in faster convergence. Less stable or slower training can result from smaller batch sizes since there may be more noise in the gradient estimates. Therefore, the trade-off between training speed and model quality can be
significantly impacted by selecting an appropriate batch size.</li>
<li>Loss function: To improve the model, different loss functions can be employed. For semantic segmentation tasks, Dice coefficients and cross-entropy are frequently used. For Cross-entropy to function, each pixel is given equal weight, whilst the Dice coefficient determines the percentage of overlap between the predicted and true masks. While Dice coefficient can perform better if there is a class imbalance or a low number of positive pixels, Cross-entropy can perform better if the bulk of pixels are in the negative class (background).</li>
<li>Optimizer: The weights of the model can be optimised using a variety of optimisation strategies. Adam is a popular optimizer that can shorten training times and boost convergence by gradually adapting the learning rate. SGD is a well-known optimisation algorithm that can be combined with momentum to reach convergence more quickly. Another widely used optimisation approach is RMSProp, which accelerates convergence by adjusting the learning rates for each parameter during training.</li>
<li>Data augmentation: Rotation, scaling, horizontal or vertical flips, colour jittering, and other data augmentation techniques can add variety to the training data and increase the model's resistance to changes in the input data. To improve the model's ability to generalise to new data and produce more varied data that the model can learn from, data augmentation can prevent overfitting.</li>
<li>Model architecture: Model designs like UNet, PSPNet, and DeepLab can enhance the model's performance for certain tasks. Popular architecture UNet can describe long-range dependencies between pixels and performs well for biomedical image segmentation. Dilated convolutions and pyramid pooling layers are used by PSPNet and DeepLab to better depict objects by capturing multi-scale information.</li>
<li>Fine-tuning: By employing pre-trained models that were trained on a distinct but similar task, transfer learning and fine-tuning can be accomplished with less time and computer resources. A pre-trained model can be fine-tuned to perform better by updating the weights of a few layers or the entire model on a specific task, making use of the pre-trained model's knowledge and optimisation.</li>
<li>Regularisation: Dropout, batch normalisation, and weight decay are regularisation methods that can increase the model's generalizability and avoid overfitting. By randomly removing (setting to zero) neurons during training, dropout aids in reducing overfitting. By normalising the input to each activation and maintaining the signal from one layer to the next, batch normalisation expedites the training process. The magnitude of the model's weights is penalised by weight decay, which results in simplicity that can help avoid overfitting.</li>
</ol>
During model training, overfitting and underfitting are frequent problems that might happen. When a model learns to fit training data too closely, it is said to have overfitted, which impairs its capacity to generalise to new data. As a result, the training set may show high accuracy, but the validation or test set may show low accuracy. Several methods, including the following, can be employed to prevent overfitting:
<ol>
  <li>Including Dropout layers prevents neurons from co-adapting and reduces overfitting by randomly removing (setting to zero) a certain proportion of them after training.</li>
  <li>Using weight decay: Weight decay adds a penalty term to the cost function, which encourages the model to choose out fewer complex weights and minimises overfitting.</li>
  <li>Early Stopping: Training can be stopped early to prevent overfitting if the model does not outperform the validation set for a predetermined number of epochs.</li>
</ol>
On the other side, underfitting happens when the model is too simple to fully represent the underlying relationships in the data, which causes a large training and validation error. Several strategies, including the following, can be employed to address underfitting:
<ol>
  <li>Expanding Model Complexity: This can be accomplished by utilising a more complicated architecture or by expanding the depth or width of the model by including additional layers or neurons.</li>
  <li>Increasing the Size and Diversity of the Training Set: By providing more instances for the model to learn from, expanding the size and diversity of the training set can also aid in reducing underfitting.</li>
  <li>Reducing Regularisation: If the regularisation strength is too high, regularisation strategies like dropout and weight decay may occasionally result in underfitting. In this situation, it may be beneficial to shorten or eliminate the regularisation term.</li>
</ol>
To determine if overfitting or underfitting is taking place, it is crucial to keep track of the training and validation losses during training. If either situation arises, it is then required to take the necessary action
<br>
<h3>Results</h3>
The Deeplab model looks to be the most accurate of the three models, according to the validation accuracy data gathered. In comparison to the UNet and FCN models, which respectively had the best validation accuracy of 0.5892 and 0.9062, Deeplab's model was able to obtain the highest validation accuracy of 0.9587. The Deeplab model's training and validation loss also consistently dropped with time and was lower than that of the competing models. It is crucial to assess the effectiveness of these models using additional measures, such as precision, recall, and f1-score. Overall, the Deeplab model seems to be the superior model based on the validation accuracy findings for the three models.<br>
Furthermore, Fully Convolutional Networks (FCNs) are deep learning models that were originally developed for image classification and then expanded to handle segmentation tasks, particularly for precise object border location. To maintain spatial information, FCNs use convolutional layers, which makes them useful for a variety of applications. FCNs provide the flexibility to handle images of various sizes, excellent precision for jobs involving image segmentation, and computational efficiency because of parameter sharing. FCNs do, however, have several shortcomings. They require a significant amount of memory for propagating activations, which, particularly with large datasets, can cause memory fatigue and out-of-memory problems. Additionally, FCNs use upsampling techniques that could blur the final image output and reduce the amount of detail.<br>
The UNet architecture provides several benefits and is frequently used for image segmentation tasks. Itâ€™s extremely effective architecture with a limited number of parameters, which makes it easier to train and less prone to overfitting, is one of its key features. Another benefit is that it uses skip connections to capture both coarse and fine-grained features. When segmenting objects with complex geometries or for image segmentation jobs with a wide range of object sizes, this can be quite helpful. The UNet concept, however, also has limitations. The fact that it might not perform as well on datasets with a significant class imbalance is one of its key drawbacks. Additionally, because it relies on downsampling and upsampling, it could find it difficult to capture small details in the image and might result in less-than-sharp segmentation borders.<br>
Modern architecture for image segmentation that offers various benefits is the DeepLab v3 ResNet50 model. One of its key benefits is the use of atrous spatial pyramid pooling (ASPP), which enables it to collect multi-scale contextual data. The ResNet50 backbone, which has been demonstrated to work effectively on a variety of computer vision tasks, is also utilised. Also demonstrated to be effective on datasets with class imbalance is the DeepLab v3 ResNet50 model.
The DeepLab v3 ResNet50 model does, however, have significant drawbacks. The fact that it has a significantly higher number of parameters than the UNet model, which can make it longer to train and more prone to overfitting, is one of its key drawbacks. Furthermore, compared to simpler models like UNet, its sophisticated architecture could make it more challenging to understand and troubleshoot.<br>
It is challenging to directly compare the effectiveness of the models. However, because it employs moresophisticated methods like ASPP and ResNet50, the DeepLab v3 ResNet50 model is anticipated to perform better on picture segmentation tasks in general. In contrast to the UNet model, it also contains a lot more parameters and can be more difficult to implement. The DeepLab v3 ResNet50 model might be a superior option for larger datasets and more challenging segmentation tasks, but the UNet model might be a viable alternative for smaller datasets or problems with a significant class imbalance. In the end, the model selection should be based on the needs of the task at hand. Moreover, due to its efficient architecture and small number of parameters, the UNet architecture is widely employed for image segmentation tasks since it is simpler to train and less prone to overfitting. When segmenting objects with complex geometries or when working with a wide variety of object sizes, UNet's use of skip connections to capture both coarse and fine-grained data can be quite helpful. Due to UNet's reliance on downsampling and upsampling approaches, it may be difficult for it to handle datasets with considerable class imbalance and to capture fine details in the image, resulting in less-than-sharp segmentation borders.<br>
DeepLab v3 ResNet50 is a paradigm for contemporary architecture for image segmentation that provides several advantages. Its usage of atrous spatial pyramid pooling (ASPP), which enables it to gather multi-scale contextual data, is one of its main advantages. The ResNet50 backbone, which has been demonstrated to be efficient on several computer vision tasks and to be able to perform well on datasets with a high class imbalance, is also used in this model. However, DeepLab v3 ResNet50 has a substantially higher number of parameters than the UNet, which can make it more difficult to troubleshoot and take longer to train as well as make it more prone to overfitting. Since each model has unique strengths and flaws it is difficult to evaluate the effectiveness of the models directly. However, because to its more advanced techniques like ASPP and ResNet50, the DeepLab v3 ResNet50 model is projected to perform better on picture segmentation tasks in general. The model has a high number of parameters, which makes it more difficult to build, but it might be a great choice for applications requiring more difficult segmentation on bigger datasets. The UNet approach, on the other hand, might be a workable substitute for issues with a severe class imbalance or smaller datasets.<br>
In summary, each of the three models, with its own set of advantages and disadvantages, is effective for picture segmentation tasks. Flexibility, precision, and parameter sharing are all features of FCNs. Better accuracy, improved feature extraction, and data augmentation are all provided by UNet. High precision and effective processing are features of DeepLab v3 ResNet50, making it particularly suitable for complicated and huge datasets. In the end, the choice of model should be determined by the requirements of the current task.<br>
<h3>Conclusion</h3>
This project provided an opportunity to explore and compare architectures for image segmentation. Through this project, we learned about the strengths and weaknesses of each semantic segmentation model structures. One thing we could have done better next time is to include a more in-depth analysis of the performance of each architecture on the specific dataset used in this project. This would have involved a more thorough evaluation of the model's accuracy, precision, recall, and F1 score. This project was a great learning experience, and it reinforced the importance of choosing the right architecture for the specific task at hand. We also gained valuable experience in implementing and training deep learning models. In addition to the points mentioned earlier, we would like to expand on the importance of hyperparameter tuning in deep learning models. Hyperparameters, such as learning rate, batch size, number of epochs, and activation functions, significantly affect the performance of the model. In this project, we could have experimented with a broader range of hyperparameter values to achieve better performance for all three models investigated in this project.<br>
Moreover, we could have delved further into data augmentation techniques, which are essential in boosting the performance of deep learning models. By applying various data augmentation techniques, such as random rotation, flipping, and shifting, the model becomes more robust and accurate. Furthermore, it is essential to have a thorough understanding of the dataset used in deep learning models. A larger dataset often leads to better performance, but a small dataset can also produce good results if it is balanced, diverse, and representative of the real-world scenario. Before training the model, it is necessary to pre-process the data carefully, by dividing it into training, validation, and testing sets, and applying normalization and scaling when necessary.<br>
Finally, we would like to mention the importance of troubleshooting common errors that arise during deep learning training. Due to the complexity of deep learning models, errors in training and testing are common. Therefore, debugging the code and identifying the root cause of the error is crucial to achieving the best results. Overall, this project provided us with a great opportunity to explore and compare different architectures for image segmentation, and we have learned valuable lessons in hyperparameter tuning, data augmentation, dataset preparation, and error troubleshooting. By applying these techniques and methods, we can continue to improve our deep learning skills and implement more robust and accurate models in future projects.

CAM101 Dataset:<br>
Julien Fauqueur, Gabriel Brostow, Roberto Cipolla, <br>
   "Assisted Video Object Labeling By Joint Tracking of Regions and Keypoints", <br>
   IEEE International Conference on Computer Vision (ICCV'2007) <br>
   Interactive Computer Vision Workshop. Rio de Janeiro, Brazil, October 2007<br>
http://www.eng.cam.ac.uk/~jf330/CamSeq01/<br>
